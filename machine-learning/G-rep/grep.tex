\documentclass[11pt]{article}
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{physics}
\usepackage{xcolor}
\usepackage{hyperref}
\definecolor{darkblue}{rgb}{0, 0, 0.5}
\hypersetup{colorlinks=true,citecolor=darkblue, linkcolor=darkblue, urlcolor=darkblue}


\title{Notes on Generalised Reparameterisation Gradient}
\author{Wilker Aziz}


\begin{document}
\maketitle

Consider the following function
\begin{equation}\label{eq:objective}
	\mathbb E_{q(z; \lambda)} \left [ \log \frac{g(z)}{q(z; \lambda)} \right]
\end{equation}
which we mean to maximise by performing stochastic gradient-based optimisation wrt parameters $\lambda$.


\section{Location-scale $q$}


I first describe the reparameterised gradient when $q(z; \lambda)$ is a location-scale family, that is, $\lambda = {\mu, C}$.
In such cases, the random variable $Z$ can be represented by transforming samples from a standard distribution $\phi(\epsilon)$ using an affine transformation:
\begin{subequations}\label{eq:h}
\begin{align}
\epsilon &= h(z; \lambda) = C^{-1} (z - \mu)  \\
z &= h^{-1}(\epsilon; \lambda) = \mu + C\epsilon ~ .
\end{align}
\end{subequations}
Note that $\phi(\epsilon)$ does not depend on $\lambda$ which are absorbed in the affine transformation.

For the sake of generality we take $z$ and $\epsilon$ to be vector valued. 
Then we write $J_{h(z; \lambda)}$ to denote the Jacobian matrix of the transformation $h(z, \lambda)$, and $J_{h^{-1}(\epsilon; \lambda)}$ to denote the Jacobian matrix of the inverse transformation.\footnote{Recall that a Jacobian matrix $\mathbf J \triangleq J_{f(x)}$ of some vector value function $f(x)$ is such that $J_{i,j} = \pdv{x_j} f_i(x)$.} 
An important property, which we will use to derive reparameterised gradients, is that the inverse of a Jacobian matrix is related to the Jacobian matrix of the inverse function by $J_{f^{-1}} \circ f(x)= J^{-1}_{f(x)}$.\footnote{The notation $J_{f^{-1}} \circ f(x)$ denotes function composition, that is, $J_{f^{-1}(y=f(x))}$ or equivalently $J_{f^{-1}(y)}\evaluated_{y=f(x)}$.}

For an invertible transformation of random variables, it holds that %We thus get the following transformed density which is equivalent to $ q(x|\lambda) $, 
\begin{equation}
q(z;\lambda) = \phi(h(z; \lambda))\abs{\det J_{h(z; \lambda)}}
\end{equation}
and therefore for the transformation in (\ref{eq:h}) we can write
\begin{equation}
q(z;\lambda) = \phi(C^{-1}(z - \mu))\abs{\det C^{-1}} 
\end{equation}
and
\begin{equation}
\phi(\epsilon) = q(\mu + C\epsilon; \lambda)\abs{\det C} ~ .
\end{equation}

Re-writing the expectation from Equation~\eqref{eq:objective} in terms of the transformed random variable we have
\begin{small}
\begin{subequations}
\begin{align}
& \int q(z;\lambda) \log \dfrac{g(z)}{q(z;\lambda)} \dd z \\
&= \int \phi(\underbrace{h(z; \lambda)}_{\epsilon}) \abs{\det J_{h(z; \lambda)}} \log \frac{g(z)}{\phi(h(z; \lambda)) \abs{\det J_{h(z; \lambda)}}} \dd z \label{eq:change-density}\\
&= \int \phi(\epsilon) \abs{\det J_{h} \circ h^{-1}(\epsilon; \lambda)} \log \frac{g(h^{-1}(\epsilon; \lambda))}{\phi(\epsilon) \abs{\det J_{h} \circ h^{-1}(\epsilon; \lambda)}} \abs{\det J_{h^{-1}(\epsilon; \lambda)}} \dd \epsilon \label{eq:change-variable} \\
&= \int \phi(\epsilon) \abs{\det J^{-1}_{h^{-1}(\epsilon; \lambda)}} \log \frac{g(h^{-1}(\epsilon; \lambda))}{\phi(\epsilon) \abs{\det J^{-1}_{h^{-1}(\epsilon; \lambda)}}} \abs{\det J_{h^{-1}(\epsilon; \lambda)}} \dd \epsilon \label{eq:inverse-theorem} \\
&= \int \phi(\epsilon) \frac{1}{\abs{\det J_{h^{-1}}(\epsilon; \lambda)}} \log \frac{g(h^{-1}(\epsilon; \lambda))\abs{\det J_{h^{-1}}(\epsilon; \lambda)}}{\phi(\epsilon)} \abs{\det J_{h^{-1}}(\epsilon; \lambda)} \dd \epsilon \label{eq:det} \\
&= \int \phi(\epsilon) \log \frac{g(h^{-1}(\epsilon; \lambda))\abs{\det J_{h^{-1}}(\epsilon; \lambda)}}{\phi(\epsilon)} \dd \epsilon \label{eq:cancel}\\
&= \int \phi(\epsilon) \log \left(g(h^{-1}(\epsilon; \lambda))\abs{\det \underbrace{J_{h^{-1}}(\epsilon; \lambda)}_{C}}\right) \dd \epsilon  - \int \phi(\epsilon) \log \phi(\epsilon) \dd \epsilon \label{eq:scale} \\
&= \mathbb E_{\phi(\epsilon)}[\log g(h^{-1}(\epsilon, \lambda))] + \log \abs{C} + \mathbb H[\phi(\epsilon)]
\end{align}
\end{subequations}
\end{small}
for which we can easily construct gradient estimates by MC sampling.

\paragraph{A digest of what happened} 

\begin{itemize}
	\item In (\ref{eq:change-density}) we applied a change of density. 
	\item In (\ref{eq:change-variable}) we applied a change of variable thus expressing every integrand as a function of $\epsilon$ rather than $z$. First, note that this calls for a change of infinitesimal volumes, i.e. $\dd z = \abs{\det J_{h^{-1}}(\epsilon, \lambda)} \dd \epsilon$. Second, note that, to express the Jacobian $J_{h(z, \lambda)}$ as a function of $\epsilon$, we used function composition.
	\item In (\ref{eq:inverse-theorem}) we used the inverse function theorem to both Jacobian terms of the kind $J_{h} \circ h^{-1}(\epsilon, \lambda)$.
	\item In (\ref{eq:det}) we use a property of determinant of invertible matrices, namely, $\det A^{-1} = \frac{1}{\det A}$.	
	\item In (\ref{eq:cancel}) the absolute determinants outside the $\log$ cancel and we are left with (\ref{eq:scale}) where we used the Jacobian of the affine transform.
	\item Note that $\phi(\epsilon)$ does not depend on $C$ and therefore the Jacobian is constant with respect to the standard distribution. 
\end{itemize}



\bibliographystyle{apalike}

\bibliography{../advi/advi.bib}

\end{document}  